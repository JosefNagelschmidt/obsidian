
1. Prove that the transpose of the transpose of a matrix is the matrix itself:
$$
(A^T)^T = A
$$
2. Given two matrices $A$ and $B$, show that sum and transposition commute:
$$
A^T + B^T = (A + B)^T
$$
3. Using the previous results, show that for any square matrix $A$, we have that $A + A^T$ is symmetric.
4. Consider three large matrices, say 
$$
A \in \mathbb{R}^{{2^{10}} \times {2^{16}}}, B \in \mathbb{R}^{{2^{16}} \times {2^{5}}}, C \in \mathbb{R}^{{2^{5}} \times {2^{14}}}
	$$
	initialized with Gaussian random variables. You want to compute the product $ABC$. Is there any difference in memory footprint and speed, depending on whether you compute $(AB) C$ or $A (BC)$? Why?